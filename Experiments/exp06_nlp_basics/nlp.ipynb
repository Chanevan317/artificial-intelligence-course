{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e06a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"\"\"\n",
    "            I am learning AI programming and the module name is Natural Language Processing in lab MA112\n",
    "            as a BTech ICT students of Marwadi University\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec055c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'AI',\n",
       " 'programming',\n",
       " 'and',\n",
       " 'the',\n",
       " 'module',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'lab',\n",
       " 'MA112',\n",
       " 'as',\n",
       " 'a',\n",
       " 'BTech',\n",
       " 'ICT',\n",
       " 'students',\n",
       " 'of',\n",
       " 'Marwadi',\n",
       " 'University']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb73705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regex: Regular Expressions, helps in pattern matching and text manipulation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4de2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation # List of punctuation characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fdb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp libraries\n",
    "import spacy # spacy, gensim, or nltk are popular libraries for NLP tasks\n",
    "nlp = spacy.load('en_core_web_sm') # Load the English language model for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c2a07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n            I am learning AI programming and the module name is Natural Language Processing in lab MA112\\n            as a BTech ICT students of Marwadi University\\n        '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3b7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text) # Process the text with spacy to create a doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4646ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc) # Check the type of the doc object, which is a spacy Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24022d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = '''\n",
    "    My name is Chan, a 21 years old young man. \n",
    "    I am a student of Marwadi University in Rajkot, India, Gujarat, pursuing BTech in ICT. \n",
    "    I am learning AI programming and the module name is Natural Language Processing in lab MA112. \n",
    "    I am here for 4 years and I am enjoying my life here. \n",
    "    I have many friends and I am learning a lot of things here. \n",
    "    I am also learning new skills and I am improving my skills day by day. \n",
    "    We have our lectures from 7:30 am to 4:30 pm. \n",
    "    I am also doing some projects and I am learning a lot of things from my projects.\n",
    "    I have one million dollars. \n",
    "    I like Samsung devices.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3df8892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = nlp(text_1) # Process the new text with spacy to create a doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a35d7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chan PERSON\n",
      "21 years old DATE\n",
      "Marwadi University ORG\n",
      "Rajkot GPE\n",
      "India GPE\n",
      "Gujarat GPE\n",
      "BTech ORG\n",
      "ICT ORG\n",
      "AI ORG\n",
      "Natural Language Processing ORG\n",
      "MA112 PRODUCT\n",
      "4 years DATE\n",
      "7:30 am to 4:30 pm TIME\n",
      "one million dollars MONEY\n",
      "Samsung ORG\n"
     ]
    }
   ],
   "source": [
    "# NER: Named Entity Recognition, helps in identifying and classifying named entities in text (like names of people, organizations, locations, etc.)\n",
    "for token in doc_1.ents:\n",
    "    print(token.text, token.label_) # Print the text and label of each named entity in the doc_1 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4541e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ->  SPACE\n",
      "My  ->  PRON\n",
      "name  ->  NOUN\n",
      "is  ->  AUX\n",
      "Chan  ->  PROPN\n",
      ",  ->  PUNCT\n",
      "a  ->  DET\n",
      "21  ->  NUM\n",
      "years  ->  NOUN\n",
      "old  ->  ADJ\n",
      "young  ->  ADJ\n",
      "man  ->  NOUN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "a  ->  DET\n",
      "student  ->  NOUN\n",
      "of  ->  ADP\n",
      "Marwadi  ->  PROPN\n",
      "University  ->  PROPN\n",
      "in  ->  ADP\n",
      "Rajkot  ->  PROPN\n",
      ",  ->  PUNCT\n",
      "India  ->  PROPN\n",
      ",  ->  PUNCT\n",
      "Gujarat  ->  PROPN\n",
      ",  ->  PUNCT\n",
      "pursuing  ->  VERB\n",
      "BTech  ->  PROPN\n",
      "in  ->  ADP\n",
      "ICT  ->  PROPN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "learning  ->  VERB\n",
      "AI  ->  PROPN\n",
      "programming  ->  NOUN\n",
      "and  ->  CCONJ\n",
      "the  ->  DET\n",
      "module  ->  NOUN\n",
      "name  ->  NOUN\n",
      "is  ->  AUX\n",
      "Natural  ->  PROPN\n",
      "Language  ->  PROPN\n",
      "Processing  ->  NOUN\n",
      "in  ->  ADP\n",
      "lab  ->  NOUN\n",
      "MA112  ->  PROPN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "here  ->  ADV\n",
      "for  ->  ADP\n",
      "4  ->  NUM\n",
      "years  ->  NOUN\n",
      "and  ->  CCONJ\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "enjoying  ->  VERB\n",
      "my  ->  PRON\n",
      "life  ->  NOUN\n",
      "here  ->  ADV\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "have  ->  VERB\n",
      "many  ->  ADJ\n",
      "friends  ->  NOUN\n",
      "and  ->  CCONJ\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "learning  ->  VERB\n",
      "a  ->  DET\n",
      "lot  ->  NOUN\n",
      "of  ->  ADP\n",
      "things  ->  NOUN\n",
      "here  ->  ADV\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "also  ->  ADV\n",
      "learning  ->  VERB\n",
      "new  ->  ADJ\n",
      "skills  ->  NOUN\n",
      "and  ->  CCONJ\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "improving  ->  VERB\n",
      "my  ->  PRON\n",
      "skills  ->  NOUN\n",
      "day  ->  NOUN\n",
      "by  ->  ADP\n",
      "day  ->  NOUN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "We  ->  PRON\n",
      "have  ->  VERB\n",
      "our  ->  PRON\n",
      "lectures  ->  NOUN\n",
      "from  ->  ADP\n",
      "7:30  ->  NUM\n",
      "am  ->  AUX\n",
      "to  ->  ADP\n",
      "4:30  ->  NUM\n",
      "pm  ->  NOUN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "also  ->  ADV\n",
      "doing  ->  VERB\n",
      "some  ->  DET\n",
      "projects  ->  NOUN\n",
      "and  ->  CCONJ\n",
      "I  ->  PRON\n",
      "am  ->  AUX\n",
      "learning  ->  VERB\n",
      "a  ->  DET\n",
      "lot  ->  NOUN\n",
      "of  ->  ADP\n",
      "things  ->  NOUN\n",
      "from  ->  ADP\n",
      "my  ->  PRON\n",
      "projects  ->  NOUN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "have  ->  VERB\n",
      "one  ->  NUM\n",
      "million  ->  NUM\n",
      "dollars  ->  NOUN\n",
      ".  ->  PUNCT\n",
      "\n",
      "      ->  SPACE\n",
      "I  ->  PRON\n",
      "like  ->  VERB\n",
      "Samsung  ->  PROPN\n",
      "devices  ->  NOUN\n",
      ".  ->  PUNCT\n",
      "\n",
      "  ->  SPACE\n"
     ]
    }
   ],
   "source": [
    "# Pos tagging: Part-of-Speech tagging, helps in identifying the grammatical category of each word in the text (like noun, verb, adjective, etc.)\n",
    "for token in doc_1:\n",
    "    print(token.text, \" -> \", token.pos_) # Print the text and part-of-speech tag of each token in the doc_1 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "460b5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can', 's', 'those', \"aren't\", 'and', \"she'll\", 'the', 'both', 'down', 'about', \"i've\", \"they're\", 'her', 'having', 'a', 'an', 'they', 'theirs', 'she', 'wouldn', 'at', 'ourselves', 'had', \"i'm\", 'has', 'before', 'other', 've', 'how', 'into', \"mightn't\", 'isn', 'or', 'once', \"they'll\", 'when', 'nor', 'be', 'with', 'mustn', 'needn', 'by', 'between', \"hadn't\", 'was', \"weren't\", 'whom', 'not', 'his', \"it's\", 'couldn', \"we'd\", 'in', 'o', 'against', 'some', 'then', 'so', \"he'd\", 'hadn', 'have', 'your', 'after', 'this', \"she'd\", \"i'll\", 'were', \"isn't\", 'shan', 'these', 'i', 'of', 'wasn', 'y', \"haven't\", 'won', 'am', 'no', 'under', 'should', 'too', 'll', 'same', \"wouldn't\", \"mustn't\", 'out', \"shan't\", 'me', 'myself', 'ain', \"couldn't\", \"don't\", \"we'll\", \"you'd\", 'for', 'my', 'its', \"she's\", 'yourself', 'because', \"didn't\", 'been', 'didn', 'herself', \"won't\", 'over', \"shouldn't\", 'than', 'there', 'here', 'very', 'don', \"it'll\", 'most', 'shouldn', 'ma', 'if', \"doesn't\", 't', 'all', \"they've\", 'do', 'haven', 'while', 'on', 'd', 'does', \"you're\", \"should've\", 'them', 'hasn', 'during', 'further', 're', 'itself', 'doesn', \"he'll\", \"hasn't\", 'which', 'now', 'again', 'doing', 'up', 'ours', \"that'll\", 'are', 'such', 'aren', 'but', 'off', 'you', 'to', 'did', 'mightn', 'why', 'from', 'hers', 'only', \"he's\", \"needn't\", 'that', 'him', \"we've\", 'our', \"i'd\", 'above', 'below', \"it'd\", 'being', \"you've\", 'what', 'own', 'himself', 'just', 'more', \"you'll\", 'where', \"we're\", \"wasn't\", 'yourselves', 'each', 'their', 'is', 'we', 'yours', 'weren', 'until', 'he', 'as', 'it', 'through', 'who', 'any', 'm', 'few', 'will', \"they'd\", 'themselves'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords: Common words that are often removed from text data during preprocessing (like \"the\", \"is\", \"in\", etc.)\n",
    "import nltk # nltk is a popular library for NLP tasks, especially for working with stopwords and other linguistic resources\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # Download the stopwords from nltk\n",
    "stop_words = set(stopwords.words('english')) # Get the set of English stopwords\n",
    "print(stop_words) # Print the set of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f161848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming vs Lemmatization: \n",
    "# Stemming is a process of reducing words to their root form by removing suffixes, \n",
    "# while lemmatization is a process of reducing words to their base form (lemma) using a dictionary and morphological analysis. \n",
    "# Stemming can sometimes produce non-words, while lemmatization always produces valid words.\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer() # Create a PorterStemmer object for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b7e840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I studied the subject named AI and then meeting Mr. Rohan tomorrow in the meeting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9c87ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  ->  i\n",
      "studied  ->  studi\n",
      "the  ->  the\n",
      "subject  ->  subject\n",
      "named  ->  name\n",
      "AI  ->  ai\n",
      "and  ->  and\n",
      "then  ->  then\n",
      "meeting  ->  meet\n",
      "Mr.  ->  mr.\n",
      "Rohan  ->  rohan\n",
      "tomorrow  ->  tomorrow\n",
      "in  ->  in\n",
      "the  ->  the\n",
      "meeting  ->  meet\n"
     ]
    }
   ],
   "source": [
    "for word in text.split():\n",
    "    print(word, \" -> \", p_stemmer.stem(word)) # Print the original word and its stemmed version using the PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1043d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  ->  PRON  ->  I\n",
      "studied  ->  VERB  ->  study\n",
      "the  ->  DET  ->  the\n",
      "subject  ->  NOUN  ->  subject\n",
      "named  ->  VERB  ->  name\n",
      "AI  ->  PROPN  ->  AI\n",
      "and  ->  CCONJ  ->  and\n",
      "then  ->  ADV  ->  then\n",
      "meeting  ->  VERB  ->  meet\n",
      "Mr.  ->  PROPN  ->  Mr.\n",
      "Rohan  ->  PROPN  ->  Rohan\n",
      "tomorrow  ->  NOUN  ->  tomorrow\n",
      "in  ->  ADP  ->  in\n",
      "the  ->  DET  ->  the\n",
      "meeting  ->  NOUN  ->  meeting\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "for token in nlp(text):\n",
    "    print(token.text, \" -> \", token.pos_, \" -> \", token.lemma_) # Print the original word and its lemmatized version using spacy's lemma_ attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de073dfe",
   "metadata": {},
   "source": [
    "task: \n",
    "take 5 sentences and apply BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303ed52",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
